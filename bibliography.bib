%!TEX root = main.tex

@article{Ronneberger_Fischer_Brox_2015,
    title={U-Net: Convolutional Networks for Biomedical Image Segmentation},
    url={http://arxiv.org/abs/1505.04597},
    author={Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
    year={2015},
    month={05}}
@misc{PET_scan,
    url={https://www.nhs.uk/conditions/pet-scan/},
    title={Positron emission tomography (PET)},
    author={NHS UK}}
@misc{Radiology_ACR,
    title={PET/CT - Positron Emission Tomography/Computed Tomography},
    url={https://www.radiologyinfo.org/en/info.cfm?pg=pet},
    author={Radiological Society of North America (RSNA) and American College of Radiology (ACR)}}
@article{Garipov_Izmailov_Podoprikhin_Vetrov_Wilson_2018, title={Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs}, url={http://arxiv.org/abs/1802.10026}, abstractNote={The loss functions of deep neural networks are complex and their geometric properties are not well understood. We show that the optima of these complex loss functions are in fact connected by simple curves, such as a polygonal chain with only one bend, over which training and test accuracy are nearly constant. We introduce a training procedure to discover these high-accuracy pathways between modes. Inspired by this new geometric insight, we also propose a new ensembling method entitled Fast Geometric Ensembling (FGE). Using FGE we can train high-performing ensembles in the time required to train a single model. We achieve improved performance compared to the recent state-of-the-art Snapshot Ensembles, on CIFAR-10 and CIFAR-100, using state-of-the-art deep residual networks. On ImageNet we improve the top-1 error-rate of a pre-trained ResNet by 0.56\% by running FGE for just 5 epochs.}, note={arXiv: 1802.10026}, journal={arXiv:1802.10026 [cs, stat]}, author={Garipov, Timur and Izmailov, Pavel and Podoprikhin, Dmitrii and Vetrov, Dmitry and Wilson, Andrew Gordon}, year={2018}, month={02}}
@article{Izmailov_Podoprikhin_Garipov_Vetrov_Wilson_2018, title={Averaging Weights Leads to Wider Optima and Better Generalization}, url={http://arxiv.org/abs/1803.05407}, abstractNote={Deep neural networks are typically trained by optimizing a loss function with an SGD variant, in conjunction with a decaying learning rate, until convergence. We show that simple averaging of multiple points along the trajectory of SGD, with a cyclical or constant learning rate, leads to better generalization than conventional training. We also show that this Stochastic Weight Averaging (SWA) procedure finds much broader optima than SGD, and approximates the recent Fast Geometric Ensembling (FGE) approach with a single model. Using SWA we achieve notable improvement in test accuracy over conventional SGD training on a range of state-of-the-art residual networks, PyramidNets, DenseNets, and Shake-Shake networks on CIFAR-10, CIFAR-100, and ImageNet. In short, SWA is extremely easy to implement, improves generalization, and has almost no computational overhead.}, note={arXiv: 1803.05407}, journal={arXiv:1803.05407 [cs, stat]}, author={Izmailov, Pavel and Podoprikhin, Dmitrii and Garipov, Timur and Vetrov, Dmitry and Wilson, Andrew Gordon}, year={2018}, month={03}}
@article{Keskar_Mudigere_Nocedal_Smelyanskiy_Tang_2016, title={On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima}, url={http://arxiv.org/abs/1609.04836}, abstractNote={The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data, say $32$-$512$ data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions - and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We discuss several strategies to attempt to help large-batch methods eliminate this generalization gap.}, note={arXiv: 1609.04836}, journal={arXiv:1609.04836 [cs, math]}, author={Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter}, year={2016}, month={09}}
@article{Smith_2015, title={Cyclical Learning Rates for Training Neural Networks}, url={http://arxiv.org/abs/1506.01186}, abstractNote={It is known that the learning rate is the most important hyper-parameter to tune for training deep neural networks. This paper describes a new method for setting the learning rate, named cyclical learning rates, which practically eliminates the need to experimentally find the best values and schedule for the global learning rates. Instead of monotonically decreasing the learning rate, this method lets the learning rate cyclically vary between reasonable boundary values. Training with cyclical learning rates instead of fixed values achieves improved classification accuracy without a need to tune and often in fewer iterations. This paper also describes a simple way to estimate “reasonable bounds” -- linearly increasing the learning rate of the network for a few epochs. In addition, cyclical learning rates are demonstrated on the CIFAR-10 and CIFAR-100 datasets with ResNets, Stochastic Depth networks, and DenseNets, and the ImageNet dataset with the AlexNet and GoogLeNet architectures. These are practical tools for everyone who trains neural networks.}, note={arXiv: 1506.01186}, journal={arXiv:1506.01186 [cs]}, author={Smith, Leslie N.}, year={2015}, month={06}}
@article{Dauphin_de_Vries_Bengio_2015, title={Equilibrated adaptive learning rates for non-convex optimization}, url={http://arxiv.org/abs/1502.04390}, abstractNote={Parameter-specific adaptive learning rate methods are computationally efficient ways to reduce the ill-conditioning problems encountered when training large deep networks. Following recent work that strongly suggests that most of the critical points encountered when training such networks are saddle points, we find how considering the presence of negative eigenvalues of the Hessian could help us design better suited adaptive learning rate schemes. We show that the popular Jacobi preconditioner has undesirable behavior in the presence of both positive and negative curvature, and present theoretical and empirical evidence that the so-called equilibration preconditioner is comparatively better suited to non-convex problems. We introduce a novel adaptive learning rate scheme, called ESGD, based on the equilibration preconditioner. Our experiments show that ESGD performs as well or better than RMSProp in terms of convergence speed, always clearly improving over plain stochastic gradient descent.}, note={arXiv: 1502.04390}, journal={arXiv:1502.04390 [cs]}, author={Dauphin, Yann N. and de Vries, Harm and Bengio, Yoshua}, year={2015}, month={02}}
@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}
@article{Loshchilov_Hutter_2016, title={SGDR: Stochastic Gradient Descent with Warm Restarts}, url={http://arxiv.org/abs/1608.03983}, abstractNote={Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradient-based optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the CIFAR-10 and CIFAR-100 datasets, where we demonstrate new state-of-the-art results at 3.14\% and 16.21\%, respectively. We also demonstrate its advantages on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset. Our source code is available at https://github.com/loshchil/SGDR}, note={arXiv: 1608.03983}, journal={arXiv:1608.03983 [cs, math]}, author={Loshchilov, Ilya and Hutter, Frank}, year={2016}, month={08}}
@article{Huang_Li_Pleiss_Liu_Hopcroft_Weinberger_2017, title={Snapshot Ensembles: Train 1, get M for free}, url={http://arxiv.org/abs/1704.00109}, abstractNote={Ensembles of neural networks are known to be much more robust and accurate than individual networks. However, training multiple deep networks for model averaging is computationally expensive. In this paper, we propose a method to obtain the seemingly contradictory goal of ensembling multiple neural networks at no additional training cost. We achieve this goal by training a single neural network, converging to several local minima along its optimization path and saving the model parameters. To obtain repeated rapid convergence, we leverage recent work on cyclic learning rate schedules. The resulting technique, which we refer to as Snapshot Ensembling, is simple, yet surprisingly effective. We show in a series of experiments that our approach is compatible with diverse network architectures and learning tasks. It consistently yields lower error rates than state-of-the-art single models at no additional training cost, and compares favorably with traditional network ensembles. On CIFAR-10 and CIFAR-100 our DenseNet Snapshot Ensembles obtain error rates of 3.4\% and 17.4\% respectively.}, note={arXiv: 1704.00109}, journal={arXiv:1704.00109 [cs]}, author={Huang, Gao and Li, Yixuan and Pleiss, Geoff and Liu, Zhuang and Hopcroft, John E. and Weinberger, Kilian Q.}, year={2017}, month={03}}
@article{Kingma_Ba_2014, title={Adam: A Method for Stochastic Optimization}, url={http://arxiv.org/abs/1412.6980}, abstractNote={We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.}, note={arXiv: 1412.6980}, journal={arXiv:1412.6980 [cs]}, author={Kingma, Diederik P. and Ba, Jimmy}, year={2014}, month={12}}
@article{Duchi_Hazan_Singer_2011,
title={Adaptive Subgradient Methods for Online Learning and Stochastic Optimization}, volume={12}, ISSN={ISSN 1533-7928}, journal={Journal of Machine Learning Research}, author={Duchi, John and Hazan, Elad and Singer, Yoram}, year={2011}, pages={2121-2159}}
@article{Zeiler_2012, title={ADADELTA: An Adaptive Learning Rate Method}, url={http://arxiv.org/abs/1212.5701}, abstractNote={We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment.}, note={arXiv: 1212.5701}, journal={arXiv:1212.5701 [cs]}, author={Zeiler, Matthew D.}, year={2012}, month={12}}
