%!TEX root = main.tex
@article{Ronneberger_Fischer_Brox_2015, title={U-Net: Convolutional Networks for Biomedical Image Segmentation}, url={http://arxiv.org/abs/1505.04597}, author={Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas}, year={2015}, month={05}}
@misc{PET_scan, url={https://www.nhs.uk/conditions/pet-scan/}, title={Positron emission tomography (PET)}, author={NHS UK}}
@misc{Radiology_ACR, title={PET/CT - Positron Emission Tomography/Computed Tomography}, url={https://www.radiologyinfo.org/en/info.cfm?pg=pet}, author={Radiological Society of North America (RSNA) and American College of Radiology (ACR)}}
@article{Garipov_Izmailov_Podoprikhin_Vetrov_Wilson_2018, title={Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs}, url={http://arxiv.org/abs/1802.10026}, abstractNote={The loss functions of deep neural networks are complex and their geometric properties are not well understood. We show that the optima of these complex loss functions are in fact connected by simple curves, such as a polygonal chain with only one bend, over which training and test accuracy are nearly constant. We introduce a training procedure to discover these high-accuracy pathways between modes. Inspired by this new geometric insight, we also propose a new ensembling method entitled Fast Geometric Ensembling (FGE). Using FGE we can train high-performing ensembles in the time required to train a single model. We achieve improved performance compared to the recent state-of-the-art Snapshot Ensembles, on CIFAR-10 and CIFAR-100, using state-of-the-art deep residual networks. On ImageNet we improve the top-1 error-rate of a pre-trained ResNet by 0.56\% by running FGE for just 5 epochs.}, note={arXiv: 1802.10026}, journal={arXiv:1802.10026 [cs, stat]}, author={Garipov, Timur and Izmailov, Pavel and Podoprikhin, Dmitrii and Vetrov, Dmitry and Wilson, Andrew Gordon}, year={2018}, month={02}}
@article{Izmailov_Podoprikhin_Garipov_Vetrov_Wilson_2018, title={Averaging Weights Leads to Wider Optima and Better Generalization}, url={http://arxiv.org/abs/1803.05407}, abstractNote={Deep neural networks are typically trained by optimizing a loss function with an SGD variant, in conjunction with a decaying learning rate, until convergence. We show that simple averaging of multiple points along the trajectory of SGD, with a cyclical or constant learning rate, leads to better generalization than conventional training. We also show that this Stochastic Weight Averaging (SWA) procedure finds much broader optima than SGD, and approximates the recent Fast Geometric Ensembling (FGE) approach with a single model. Using SWA we achieve notable improvement in test accuracy over conventional SGD training on a range of state-of-the-art residual networks, PyramidNets, DenseNets, and Shake-Shake networks on CIFAR-10, CIFAR-100, and ImageNet. In short, SWA is extremely easy to implement, improves generalization, and has almost no computational overhead.}, note={arXiv: 1803.05407}, journal={arXiv:1803.05407 [cs, stat]}, author={Izmailov, Pavel and Podoprikhin, Dmitrii and Garipov, Timur and Vetrov, Dmitry and Wilson, Andrew Gordon}, year={2018}, month={03}}
@article{Keskar_Mudigere_Nocedal_Smelyanskiy_Tang_2016, title={On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima}, url={http://arxiv.org/abs/1609.04836}, abstractNote={The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data, say $32$-$512$ data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions - and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We discuss several strategies to attempt to help large-batch methods eliminate this generalization gap.}, note={arXiv: 1609.04836}, journal={arXiv:1609.04836 [cs, math]}, author={Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter}, year={2016}, month={09}}
@article{Smith_2015, title={Cyclical Learning Rates for Training Neural Networks}, url={http://arxiv.org/abs/1506.01186}, abstractNote={It is known that the learning rate is the most important hyper-parameter to tune for training deep neural networks. This paper describes a new method for setting the learning rate, named cyclical learning rates, which practically eliminates the need to experimentally find the best values and schedule for the global learning rates. Instead of monotonically decreasing the learning rate, this method lets the learning rate cyclically vary between reasonable boundary values. Training with cyclical learning rates instead of fixed values achieves improved classification accuracy without a need to tune and often in fewer iterations. This paper also describes a simple way to estimate “reasonable bounds” -- linearly increasing the learning rate of the network for a few epochs. In addition, cyclical learning rates are demonstrated on the CIFAR-10 and CIFAR-100 datasets with ResNets, Stochastic Depth networks, and DenseNets, and the ImageNet dataset with the AlexNet and GoogLeNet architectures. These are practical tools for everyone who trains neural networks.}, note={arXiv: 1506.01186}, journal={arXiv:1506.01186 [cs]}, author={Smith, Leslie N.}, year={2015}, month={06}}
@article{Dauphin_de_Vries_Bengio_2015, title={Equilibrated adaptive learning rates for non-convex optimization}, url={http://arxiv.org/abs/1502.04390}, abstractNote={Parameter-specific adaptive learning rate methods are computationally efficient ways to reduce the ill-conditioning problems encountered when training large deep networks. Following recent work that strongly suggests that most of the critical points encountered when training such networks are saddle points, we find how considering the presence of negative eigenvalues of the Hessian could help us design better suited adaptive learning rate schemes. We show that the popular Jacobi preconditioner has undesirable behavior in the presence of both positive and negative curvature, and present theoretical and empirical evidence that the so-called equilibration preconditioner is comparatively better suited to non-convex problems. We introduce a novel adaptive learning rate scheme, called ESGD, based on the equilibration preconditioner. Our experiments show that ESGD performs as well or better than RMSProp in terms of convergence speed, always clearly improving over plain stochastic gradient descent.}, note={arXiv: 1502.04390}, journal={arXiv:1502.04390 [cs]}, author={Dauphin, Yann N. and de Vries, Harm and Bengio, Yoshua}, year={2015}, month={02}}
@book{Goodfellow-et-al-2016, title={Deep Learning}, author={Ian Goodfellow and Yoshua Bengio and Aaron Courville}, publisher={MIT Press}, note={\url{http://www.deeplearningbook.org}}, year={2016}}
@article{Loshchilov_Hutter_2016, title={SGDR: Stochastic Gradient Descent with Warm Restarts}, url={http://arxiv.org/abs/1608.03983}, abstractNote={Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradient-based optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the CIFAR-10 and CIFAR-100 datasets, where we demonstrate new state-of-the-art results at 3.14\% and 16.21\%, respectively. We also demonstrate its advantages on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset. Our source code is available at https://github.com/loshchil/SGDR}, note={arXiv: 1608.03983}, journal={arXiv:1608.03983 [cs, math]}, author={Loshchilov, Ilya and Hutter, Frank}, year={2016}, month={08}}
@article{Huang_Li_Pleiss_Liu_Hopcroft_Weinberger_2017, title={Snapshot Ensembles: Train 1, get M for free}, url={http://arxiv.org/abs/1704.00109}, abstractNote={Ensembles of neural networks are known to be much more robust and accurate than individual networks. However, training multiple deep networks for model averaging is computationally expensive. In this paper, we propose a method to obtain the seemingly contradictory goal of ensembling multiple neural networks at no additional training cost. We achieve this goal by training a single neural network, converging to several local minima along its optimization path and saving the model parameters. To obtain repeated rapid convergence, we leverage recent work on cyclic learning rate schedules. The resulting technique, which we refer to as Snapshot Ensembling, is simple, yet surprisingly effective. We show in a series of experiments that our approach is compatible with diverse network architectures and learning tasks. It consistently yields lower error rates than state-of-the-art single models at no additional training cost, and compares favorably with traditional network ensembles. On CIFAR-10 and CIFAR-100 our DenseNet Snapshot Ensembles obtain error rates of 3.4\% and 17.4\% respectively.}, note={arXiv: 1704.00109}, journal={arXiv:1704.00109 [cs]}, author={Huang, Gao and Li, Yixuan and Pleiss, Geoff and Liu, Zhuang and Hopcroft, John E. and Weinberger, Kilian Q.}, year={2017}, month={03}}
@article{Kingma_Ba_2014, title={Adam: A Method for Stochastic Optimization}, url={http://arxiv.org/abs/1412.6980}, abstractNote={We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.}, note={arXiv: 1412.6980}, journal={arXiv:1412.6980 [cs]}, author={Kingma, Diederik P. and Ba, Jimmy}, year={2014}, month={12}}
@article{Zeiler_2012, title={ADADELTA: An Adaptive Learning Rate Method}, url={http://arxiv.org/abs/1212.5701}, abstractNote={We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment.}, note={arXiv: 1212.5701}, journal={arXiv:1212.5701 [cs]}, author={Zeiler, Matthew D.}, year={2012}, month={12}}
@article{McCulloch_Pitts_1943, title={A logical calculus of the ideas immanent in nervous activity}, volume={5}, ISSN={0007-4985, 1522-9602}, DOI={10.1007/BF02478259}, abstractNote={Because of the “all-or-none” character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.}, number={4}, journal={The bulletin of mathematical biophysics}, author={McCulloch, Warren S. and Pitts, Walter}, year={1943}, month={12}, pages={115-133}}
@article{Rosenblatt_1958, title={The perceptron: A probabilistic model for information storage and organization in the brain.}, volume={65}, ISSN={1939-1471(Electronic),0033-295X(Print)}, DOI={10.1037/h0042519}, abstractNote={To answer the questions of how information about the physical world is sensed, in what form is information remembered, and how does information retained in memory influence recognition and behavior, a theory is developed for a hypothetical nervous system called a perceptron. The theory serves as a bridge between biophysics and psychology. It is possible to predict learning curves from neurological variables and vice versa. The quantitative statistical approach is fruitful in the understanding of the organization of cognitive systems. 18 references. (PsycINFO Database Record (c) 2016 APA, all rights reserved)}, number={6}, journal={Psychological Review}, author={Rosenblatt, F.}, year={1958}, pages={386-408}}
@book{Minsky_Papert_1988, title={Perceptrons: An Introduction to Computational Geometry}, ISBN={978-0-262-34394-7}, abstractNote={The first systematic study of parallelism in computation by two pioneers in the field.Reissue of the 1988 Expanded Edition with a new foreword by Léon BottouIn 1969, ten years after the discovery of the perceptron—which showed that a machine could be taught to perform certain tasks using examples—Marvin Minsky and Seymour Papert published Perceptrons, their analysis of the computational capabilities of perceptrons for specific tasks. As Léon Bottou writes in his foreword to this edition, “Their rigorous work and brilliant technique does not make the perceptron look very good.” Perhaps as a result, research turned away from the perceptron. Then the pendulum swung back, and machine learning became the fastest-growing field in computer science. Minsky and Papert’s insistence on its theoretical foundations is newly relevant.Perceptrons—the first systematic study of parallelism in computation—marked a historic turn in artificial intelligence, returning to the idea that intelligence might emerge from the activity of networks of neuron-like entities. Minsky and Papert provided mathematical analysis that showed the limitations of a class of computing machines that could be considered as models of the brain. Minsky and Papert added a new chapter in 1987 in which they discuss the state of parallel computers, and note a central theoretical challenge: reaching a deeper understanding of how “objects” or “agents” with individuality can emerge in a network. Progress in this area would link connectionism with what the authors have called “society theories of mind.”}, note={Google-Books-ID: MrQ5DwAAQBAJ}, publisher={MIT Press}, author={Minsky, Marvin and Papert, Seymour A.}, year={1988}}
@book{Rosenblatt_1957, title={The Perceptron - A Perceiving and Recognizing Automaton}, url={https://blogs.umass.edu/brain-wars/files/2016/03/rosenblatt-1957.pdf}, author={Rosenblatt, F.}, year={1957}, month={01}}
@article{Rumelhart_Hinton_Williams_1986, title={Learning representations by back-propagating errors}, volume={323}, ISSN={1476-4687}, DOI={10.1038/323533a0}, abstractNote={We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal ‘hidden’ units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.}, number={6088}, journal={Nature}, author={Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.}, year={1986}, month={08}, pages={533-536}}
@article{Cicek_Abdulkadir_Lienkamp_Brox_Ronneberger_2016, title={3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation}, url={http://arxiv.org/abs/1606.06650}, abstractNote={This paper introduces a network for volumetric segmentation that learns from sparsely annotated volumetric images. We outline two attractive use cases of this method: (1) In a semi-automated setup, the user annotates some slices in the volume to be segmented. The network learns from these sparse annotations and provides a dense 3D segmentation. (2) In a fully-automated setup, we assume that a representative, sparsely annotated training set exists. Trained on this data set, the network densely segments new volumetric images. The proposed network extends the previous u-net architecture from Ronneberger et al. by replacing all 2D operations with their 3D counterparts. The implementation performs on-the-fly elastic deformations for efficient data augmentation during training. It is trained end-to-end from scratch, i.e., no pre-trained network is required. We test the performance of the proposed method on a complex, highly variable 3D structure, the Xenopus kidney, and achieve good results for both use cases.}, note={arXiv: 1606.06650}, journal={arXiv:1606.06650 [cs]}, author={Cicek, Ozgun and Abdulkadir, Ahmed and Lienkamp, Soeren S. and Brox, Thomas and Ronneberger, Olaf}, year={2016}, month={06}}
@article{Liu_Gadepalli_Norouzi_Dahl_Kohlberger_Boyko_Venugopalan_Timofeev_Nelson_Corrado_et_al_2017, title={Detecting Cancer Metastases on Gigapixel Pathology Images}, url={http://arxiv.org/abs/1703.02442}, abstractNote={Each year, the treatment decisions for more than 230,000 breast cancer patients in the U.S. hinge on whether the cancer has metastasized away from the breast. Metastasis detection is currently performed by pathologists reviewing large expanses of biological tissues. This process is labor intensive and error-prone. We present a framework to automatically detect and localize tumors as small as 100 x 100 pixels in gigapixel microscopy images sized 100,000 x 100,000 pixels. Our method leverages a convolutional neural network (CNN) architecture and obtains state-of-the-art results on the Camelyon16 dataset in the challenging lesion-level tumor detection task. At 8 false positives per image, we detect 92.4\% of the tumors, relative to 82.7\% by the previous best automated approach. For comparison, a human pathologist attempting exhaustive search achieved 73.2\% sensitivity. We achieve image-level AUC scores above 97\% on both the Camelyon16 test set and an independent set of 110 slides. In addition, we discover that two slides in the Camelyon16 training set were erroneously labeled normal. Our approach could considerably reduce false negative rates in metastasis detection.}, note={arXiv: 1703.02442}, journal={arXiv:1703.02442 [cs]}, author={Liu, Yun and Gadepalli, Krishna and Norouzi, Mohammad and Dahl, George E. and Kohlberger, Timo and Boyko, Aleksey and Venugopalan, Subhashini and Timofeev, Aleksei and Nelson, Philip Q. and Corrado, Greg S. and et al.}, year={2017}, month={03}}
@article{Esteva_Kuprel_Novoa_Ko_Swetter_Blau_Thrun_2017, title={Dermatologist-level classification of skin cancer with deep neural networks}, volume={542}, ISSN={1476-4687}, DOI={10.1038/nature21056}, abstractNote={<p>Andre Esteva et al. used 129,450 clinical images of skin disease to train a deep convolutional neural network to classify skin lesions. The result is an algorithm that can classify lesions from photographic images similar to those taken with a mobile phone. The accuracy of the system in detecting malignant melanomas and carcinomas matched that of trained dermatologists. The authors suggest that the technique could be used outside the clinic as a visual screen for cancer.</p>}, number={7639}, journal={Nature}, author={Esteva, Andre and Kuprel, Brett and Novoa, Roberto A. and Ko, Justin and Swetter, Susan M. and Blau, Helen M. and Thrun, Sebastian}, year={2017}, month={01}, pages={115}}
@article{Iglovikov_Shvets_2018, title={TernausNet: U-Net with VGG11 Encoder Pre-Trained on ImageNet for Image Segmentation}, url={http://arxiv.org/abs/1801.05746}, abstractNote={Pixel-wise image segmentation is demanding task in computer vision. Classical U-Net architectures composed of encoders and decoders are very popular for segmentation of medical images, satellite images etc. Typically, neural network initialized with weights from a network pre-trained on a large data set like ImageNet shows better performance than those trained from scratch on a small dataset. In some practical applications, particularly in medicine and traffic safety, the accuracy of the models is of utmost importance. In this paper, we demonstrate how the U-Net type architecture can be improved by the use of the pre-trained encoder. Our code and corresponding pre-trained weights are publicly available at https://github.com/ternaus/TernausNet. We compare three weight initialization schemes: LeCun uniform, the encoder with weights from VGG11 and full network trained on the Carvana dataset. This network architecture was a part of the winning solution (1st out of 735) in the Kaggle: Carvana Image Masking Challenge.}, note={arXiv: 1801.05746}, journal={arXiv:1801.05746 [cs]}, author={Iglovikov, Vladimir and Shvets, Alexey}, year={2018}, month={01}}
@article{Hinton_Sabour_Frosst_2018, title={MATRIX CAPSULES WITH EM ROUTING}, abstractNote={A capsule is a group of neurons whose outputs represent different properties of the same entity. Each layer in a capsule network contains many capsules. We describe a version of capsules in which each capsule has a logistic unit to represent the presence of an entity and a 4x4 matrix which could learn to represent the relationship between that entity and the viewer (the pose). A capsule in one layer votes for the pose matrix of many different capsules in the layer above by multiplying its own pose matrix by trainable viewpoint-invariant transformation matrices that could learn to represent part-whole relationships. Each of these votes is weighted by an assignment coefﬁcient. These coefﬁcients are iteratively updated for each image using the Expectation-Maximization algorithm such that the output of each capsule is routed to a capsule in the layer above that receives a cluster of similar votes. The transformation matrices are trained discriminatively by backpropagating through the unrolled iterations of EM between each pair of adjacent capsule layers. On the smallNORB benchmark, capsules reduce the number of test errors by 45\% compared to the state-of-the-art. Capsules also show far more resistance to white box adversarial attacks than our baseline convolutional neural network.}, author={Hinton, Geoffrey and Sabour, Sara and Frosst, Nicholas}, year={2018}, pages={15}}
@article{Sabour_Frosst_Hinton_2017, title={Dynamic Routing Between Capsules}, url={http://arxiv.org/abs/1710.09829}, abstractNote={A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or an object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.}, note={arXiv: 1710.09829}, journal={arXiv:1710.09829 [cs]}, author={Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E.}, year={2017}, month={08}}
@article{Lippmann_1987, title={An introduction to computing with neural nets}, volume={4}, ISSN={0740-7467}, DOI={10.1109/MASSP.1987.1165576}, abstractNote={Artificial neural net models have been studied for many years in the hope of achieving human-like performance in the fields of speech and image recognition. These models are composed of many nonlinear computational elements operating in parallel and arranged in patterns reminiscent of biological neural nets. Computational elements or nodes are connected via weights that are typically adapted during use to improve performance. There has been a recent resurgence in the field of artificial neural nets caused by new net topologies and algorithms, analog VLSI implementation techniques, and the belief that massive parallelism is essential for high performance speech and image recognition. This paper provides an introduction to the field of artificial neural nets by reviewing six important neural net models that can be used for pattern classification. These nets are highly parallel building blocks that illustrate neural net components and design principles and can be used to construct more complex systems. In addition to describing these nets, a major emphasis is placed on exploring how some existing classification and clustering algorithms can be performed using simple neuron-like components. Single-layer nets can implement algorithms required by Gaussian maximum-likelihood classifiers and optimum minimum-error classifiers for binary patterns corrupted by noise. More generally, the decision regions required by any classification algorithm can be generated in a straightforward manner by three-layer feed-forward nets.}, number={2}, journal={IEEE ASSP Magazine}, author={Lippmann, R.}, year={1987}, month={04}, pages={4-22}}
@article{Litjens_Kooi_Bejnordi_Setio_Ciompi_Ghafoorian_van_der_Laak_van_Ginneken_Sanchez_2017, title={A Survey on Deep Learning in Medical Image Analysis}, volume={42}, ISSN={13618415}, DOI={10.1016/j.media.2017.07.005}, abstractNote={Deep learning algorithms, in particular convolutional networks, have rapidly become a methodology of choice for analyzing medical images. This paper reviews the major deep learning concepts pertinent to medical image analysis and summarizes over 300 contributions to the field, most of which appeared in the last year. We survey the use of deep learning for image classification, object detection, segmentation, registration, and other tasks and provide concise overviews of studies per application area. Open challenges and directions for future research are discussed.}, note={arXiv: 1702.05747}, journal={Medical Image Analysis}, author={Litjens, Geert and Kooi, Thijs and Bejnordi, Babak Ehteshami and Setio, Arnaud Arindra Adiyoso and Ciompi, Francesco and Ghafoorian, Mohsen and van der Laak, Jeroen A. W. M. and van Ginneken, Bram and Sanchez, Clara I.}, year={2017}, month={12}, pages={60-88}}
@article{Kawaguchi_2016, title={Deep Learning without Poor Local Minima}, url={http://arxiv.org/abs/1605.07110}, abstractNote={In this paper, we prove a conjecture published in 1989 and also partially address an open problem announced at the Conference on Learning Theory (COLT) 2015. With no unrealistic assumption, we first prove the following statements for the squared loss function of deep linear neural networks with any depth and any widths: 1) the function is non-convex and non-concave, 2) every local minimum is a global minimum, 3) every critical point that is not a global minimum is a saddle point, and 4) there exist “bad” saddle points (where the Hessian has no negative eigenvalue) for the deeper networks (with more than three layers), whereas there is no bad saddle point for the shallow networks (with three layers). Moreover, for deep nonlinear neural networks, we prove the same four statements via a reduction to a deep linear model under the independence assumption adopted from recent work. As a result, we present an instance, for which we can answer the following question: how difficult is it to directly train a deep model in theory? It is more difficult than the classical machine learning models (because of the non-convexity), but not too difficult (because of the nonexistence of poor local minima). Furthermore, the mathematically proven existence of bad saddle points for deeper models would suggest a possible open problem. We note that even though we have advanced the theoretical foundations of deep learning and non-convex optimization, there is still a gap between theory and practice.}, note={arXiv: 1605.07110}, journal={arXiv:1605.07110 [cs, math, stat]}, author={Kawaguchi, Kenji}, year={2016}, month={05}}
@inproceedings{Caruana_Niculescu-Mizil_Crew_Ksikes_2004, place={New York, NY, USA}, series={ICML '04}, title={Ensemble Selection from Libraries of Models}, ISBN={978-1-58113-838-2}, url={http://doi.acm.org/10.1145/1015330.1015432}, DOI={10.1145/1015330.1015432}, abstractNote={We present a method for constructing ensembles from libraries of thousands of models. Model libraries are generated using different learning algorithms and parameter settings. Forward stepwise selection is used to add to the ensemble the models that maximize its performance. Ensemble selection allows ensembles to be optimized to performance metric such as accuracy, cross entropy, mean precision, or ROC Area. Experiments with seven test problems and ten metrics demonstrate the benefit of ensemble selection.}, booktitle={Proceedings of the Twenty-first International Conference on Machine Learning}, publisher={ACM}, author={Caruana, Rich and Niculescu-Mizil, Alexandru and Crew, Geoff and Ksikes, Alex}, year={2004}, pages={18-}, collection={ICML '04}}
@article{Duchi_Hazan_Singer_2011, title={Adaptive Subgradient Methods for Online Learning and Stochastic Optimization}, volume={12}, ISSN={ISSN 1533-7928}, journal={Journal of Machine Learning Research}, author={Duchi, John and Hazan, Elad and Singer, Yoram}, year={2011}, pages={2121-2159}}
@inbook{Han_Moraga_1995, series={Lecture Notes in Computer Science}, title={The influence of the sigmoid function parameters on the speed of backpropagation learning}, ISBN={978-3-540-59497-0}, url={https://link.springer.com/chapter/10.1007/3-540-59497-3_175}, DOI={10.1007/3-540-59497-3_175}, abstractNote={Sigmoid function is the most commonly known function used in feed forward neural networks because of its nonlinearity and the computational simplicity of its derivative. In this paper we discuss a variant sigmoid function with three parameters that denote the dynamic range, symmetry and slope of the function respectively. We illustrate how these parameters influence the speed of backpropagation learning and introduce a hybrid sigmoidal network with different parameter configuration in different layers. By regulating and modifying the sigmoid function parameter configuration in different layers the error signal problem, oscillation problem and asymmetrical input problem can be reduced. To compare the learning capabilities and the learning rate of the hybrid sigmoidal networks with the conventional networks we have tested the two-spirals benchmark that is known to be a very difficult task for backpropagation and their relatives.}, booktitle={From Natural to Artificial Neural Computation}, publisher={Springer, Berlin, Heidelberg}, author={Han, Jun and Moraga, Claudio}, year={1995}, month={06}, pages={195-201}, collection={Lecture Notes in Computer Science}}
@book{Jarrett_Kavukcuoglu_Lecun, title={What is the Best Multi-Stage Architecture for Object Recognition?}, abstractNote={In many recent object recognition systems, feature extraction stages are generally composed of a filter bank, a non-linear transformation, and some sort of feature pooling layer. Most systems use only one stage of feature extraction in which the filters are hard-wired, or two stages where the filters in one or both stages are learned in supervised or unsupervised mode. This paper addresses three questions: 1. How does the non-linearities that follow the filter banks influence the recognition accuracy? 2. does learning the filter banks in an unsupervised or supervised manner improve the performance over random filters or hardwired filters? 3. Is there any advantage to using an architecture with two stages of feature extraction, rather than one? We show that using non-linearities that include rectification and local contrast normalization is the single most important ingredient for good accuracy on object recognition benchmarks. We show that two stages of feature extraction yield better accuracy than one. Most surprisingly, we show that a two-stage system with random filters can yield almost 63 \% recognition rate on Caltech-101, provided that the proper non-linearities and pooling layers are used. Finally, we show that with supervised refinement, the system achieves state-of-the-art performance on NORB dataset (5.6\%) and unsupervised pre-training followed by supervised refinement produces good accuracy on Caltech-101 (> 65\%), and the lowest known error rate on the undistorted, unprocessed MNIST dataset (0.53\%). 1.}, author={Jarrett, Kevin and Kavukcuoglu, Koray and Lecun, Yann}}
@article{Manessi_Rozza_2018, title={Learning Combinations of Activation Functions}, url={http://arxiv.org/abs/1801.09403}, abstractNote={In the last decade, an active area of research has been devoted to design novel activation functions that are able to help deep neural networks to converge, obtaining better performance. The training procedure of these architectures usually involves optimization of the weights of their layers only, while non-linearities are generally pre-specified and their (possible) parameters are usually considered as hyper-parameters to be tuned manually. In this paper, we introduce two approaches to automatically learn different combinations of base activation functions (such as the identity function, ReLU, and tanh) during the training phase. We present a thorough comparison of our novel approaches with well-known architectures (such as LeNet-5, AlexNet, and ResNet-56) on three standard datasets (Fashion-MNIST, CIFAR-10, and ILSVRC-2012), showing substantial improvements in the overall performance, such as an increase in the top-1 accuracy for AlexNet on ILSVRC-2012 of 3.01 percentage points.}, note={arXiv: 1801.09403}, journal={arXiv:1801.09403 [cs]}, author={Manessi, Franco and Rozza, Alessandro}, year={2018}, month={01}}
@inbook{LeCun_Bottou_Orr_Muller_1998, series={Lecture Notes in Computer Science}, title={Efficient BackProp}, ISBN={978-3-540-65311-0}, url={https://link.springer.com/chapter/10.1007/3-540-49430-8_2}, DOI={10.1007/3-540-49430-8_2}, abstractNote={The convergence of back-propagation learning is analyzed so as to explain common phenomenon observedb y practitioners. Many undesirable behaviors of backprop can be avoided with tricks that are rarely exposedin serious technical publications. This paper gives some of those tricks, ando.ers explanations of why they work. Many authors have suggested that second-order optimization methods are advantageous for neural net training. It is shown that most “classical” second-order methods are impractical for large neural networks. A few methods are proposed that do not have these limitations.}, booktitle={Neural Networks: Tricks of the Trade}, publisher={Springer, Berlin, Heidelberg}, author={LeCun, Yann and Bottou, Leon and Orr, Genevieve B. and Muller, Klaus-Robert}, year={1998}, pages={9-50}, collection={Lecture Notes in Computer Science}}
@article{Ramachandran_Zoph_Le_2017, title={Searching for Activation Functions}, url={http://arxiv.org/abs/1710.05941}, abstractNote={The choice of activation functions in deep networks has a significant effect on the training dynamics and task performance. Currently, the most successful and widely-used activation function is the Rectified Linear Unit (ReLU). Although various hand-designed alternatives to ReLU have been proposed, none have managed to replace it due to inconsistent gains. In this work, we propose to leverage automatic search techniques to discover new activation functions. Using a combination of exhaustive and reinforcement learning-based search, we discover multiple novel activation functions. We verify the effectiveness of the searches by conducting an empirical evaluation with the best discovered activation function. Our experiments show that the best discovered activation function, $f(x) = x cdot text{sigmoid}(beta x)$, which we name Swish, tends to work better than ReLU on deeper models across a number of challenging datasets. For example, simply replacing ReLUs with Swish units improves top-1 classification accuracy on ImageNet by 0.9\% for Mobile NASNet-A and 0.6\% for Inception-ResNet-v2. The simplicity of Swish and its similarity to ReLU make it easy for practitioners to replace ReLUs with Swish units in any neural network.}, note={arXiv: 1710.05941}, journal={arXiv:1710.05941 [cs]}, author={Ramachandran, Prajit and Zoph, Barret and Le, Quoc V.}, year={2017}, month={09}}
@article{Stephens_Lee_Faghri_Campbell_Zhai_Efron_Iyer_Schatz_Sinha_Robinson_2015, title={Big Data: Astronomical or Genomical?}, volume={13}, ISSN={1545-7885}, DOI={10.1371/journal.pbio.1002195}, abstractNote={Genomics is a Big Data science and is going to get much bigger, very soon, but it is not known whether the needs of genomics will exceed other Big Data domains. Projecting to the year 2025, we compared genomics with three other major generators of Big Data: astronomy, YouTube, and Twitter. Our estimates show that genomics is a “four-headed beast”—it is either on par with or the most demanding of the domains analyzed here in terms of data acquisition, storage, distribution, and analysis. We discuss aspects of new technologies that will need to be developed to rise up and meet the computational challenges that genomics poses for the near future. Now is the time for concerted, community-wide planning for the “genomical” challenges of the next decade.}, number={7}, journal={PLOS Biology}, author={Stephens, Zachary D. and Lee, Skylar Y. and Faghri, Faraz and Campbell, Roy H. and Zhai, Chengxiang and Efron, Miles J. and Iyer, Ravishankar and Schatz, Michael C. and Sinha, Saurabh and Robinson, Gene E.}, year={2015}, month={07}, pages={e1002195}}
@article{Enzinger_Mayer_2003, title={Esophageal Cancer}, volume={349}, ISSN={0028-4793}, DOI={10.1056/NEJMra035010}, abstractNote={Cancers arising from the esophagus, including the gastroesophageal junction, are relatively uncommon in the United States - the lifetime risk of this cancer is 0.8 percent for men and 0.3 percent for women, and it increases with age. The presentation is insidious; at diagnosis, more than 50 percent of patients have either unresectable cancer or radiographically visible metastases, rendering management problematic. This review discusses the pathogenesis of esophageal cancer, as well as the clinical presentation, treatment, and prognosis.}, number={23}, journal={New England Journal of Medicine}, author={Enzinger, Peter C. and Mayer, Robert J.}, year={2003}, month={12}, pages={2241-2252}}
@article{Finlayson_Chung_Kohane_Beam_2018, title={Adversarial Attacks Against Medical Deep Learning Systems}, url={http://arxiv.org/abs/1804.05296}, abstractNote={The discovery of adversarial examples has raised concerns about the practical deployment of deep learning systems. In this paper, we argue that the field of medicine may be uniquely susceptible to adversarial attacks, both in terms of monetary incentives and technical vulnerability. To this end, we outline the healthcare economy and the incentives it creates for fraud, we extend adversarial attacks to three popular medical imaging tasks, and we provide concrete examples of how and why such attacks could be realistically carried out. For each of our representative medical deep learning classifiers, both white and black box attacks were highly successful. We urge caution in deploying deep learning systems in clinical settings, and encourage the machine learning community to further investigate the domain-specific characteristics of medical learning systems.}, note={arXiv: 1804.05296}, journal={arXiv:1804.05296 [cs, stat]}, author={Finlayson, Samuel G. and Chung, Hyung Won and Kohane, Isaac S. and Beam, Andrew L.}, year={2018}, month={04}}
@article{Ching_Himmelstein_Beaulieu-Jones_Kalinin_Do_Way_Ferrero_Agapow_Zietz_Hoffman_et_al_2018, title={Opportunities and obstacles for deep learning in biology and medicine}, volume={15}, ISSN={1742-5689, 1742-5662}, DOI={10.1098/rsif.2017.0387}, abstractNote={Deep learning describes a class of machine learning algorithms that are capable of combining raw inputs into layers of intermediate features. These algorithms have recently shown impressive results across a variety of domains. Biology and medicine are data-rich disciplines, but the data are complex and often ill-understood. Hence, deep learning techniques may be particularly well suited to solve problems of these fields. We examine applications of deep learning to a variety of biomedical problems—patient classification, fundamental biological processes and treatment of patients—and discuss whether deep learning will be able to transform these tasks or if the biomedical sphere poses unique challenges. Following from an extensive literature review, we find that deep learning has yet to revolutionize biomedicine or definitively resolve any of the most pressing challenges in the field, but promising advances have been made on the prior state of the art. Even though improvements over previous baselines have been modest in general, the recent progress indicates that deep learning methods will provide valuable means for speeding up or aiding human investigation. Though progress has been made linking a specific neural network’s prediction to input features, understanding how users should interpret these models to make testable hypotheses about the system under study remains an open challenge. Furthermore, the limited amount of labelled data for training presents problems in some domains, as do legal and privacy constraints on work with sensitive health records. Nonetheless, we foresee deep learning enabling changes at both bench and bedside with the potential to transform several areas of biology and medicine.}, number={141}, journal={Journal of The Royal Society Interface}, author={Ching, Travers and Himmelstein, Daniel S. and Beaulieu-Jones, Brett K. and Kalinin, Alexandr A. and Do, Brian T. and Way, Gregory P. and Ferrero, Enrico and Agapow, Paul-Michael and Zietz, Michael and Hoffman, Michael M. and et al.}, year={2018}, month={04}, pages={20170387}}
@article{Egmont-Petersen_de_Ridder_Handels_2002, title={Image processing with neural networks-a review}, volume={35}, ISSN={0031-3203}, DOI={10.1016/S0031-3203(01)00178-9}, abstractNote={We review more than 200 applications of neural networks in image processing and discuss the present and possible future role of neural networks, especially feed-forward neural networks, Kohonen feature maps and Hopfield neural networks. The various applications are categorised into a novel two-dimensional taxonomy for image processing algorithms. One dimension specifies the type of task performed by the algorithm: preprocessing, data reduction/feature extraction, segmentation, object recognition, image understanding and optimisation. The other dimension captures the abstraction level of the input data processed by the algorithm: pixel-level, local feature-level, structure-level, object-level, object-set-level and scene characterisation. Each of the six types of tasks poses specific constraints to a neural-based approach. These specific conditions are discussed in detail. A synthesis is made of unresolved problems related to the application of pattern recognition techniques in image processing and specifically to the application of neural networks. Finally, we present an outlook into the future application of neural networks and relate them to novel developments.}, number={10}, journal={Pattern Recognition}, author={Egmont-Petersen, M. and de Ridder, D. and Handels, H.}, year={2002}, month={10}, pages={2279-2301}}
@article{Russakovsky_Deng_Su_Krause_Satheesh_Ma_Huang_Karpathy_Khosla_Bernstein_et_al_2015, title={ImageNet Large Scale Visual Recognition Challenge}, volume={115}, ISSN={0920-5691, 1573-1405}, DOI={10.1007/s11263-015-0816-y}, abstractNote={The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5 years of the challenge, and propose future directions and improvements.}, number={3}, journal={International Journal of Computer Vision}, author={Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and et al.}, year={2015}, month={12}, pages={211-252}}
@misc{Camelyon16, url={https://camelyon16.grand-challenge.org/}, title={Camelyon 2016}}
@misc{chollet2015keras, title={Keras}, author={Chollet, Francois and others}, year={2015}, howpublished={\url{https://keras.io}}}
@misc{tensorflow2015-whitepaper, title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems}, url={https://www.tensorflow.org/}, note={Software available from tensorflow.org}, author={ Martin~Abadi and Ashish~Agarwal and Paul~Barham and Eugene~Brevdo and Zhifeng~Chen and Craig~Citro and Greg~S.~Corrado and Andy~Davis and Jeffrey~Dean and Matthieu~Devin and Sanjay~Ghemawat and Ian~Goodfellow and Andrew~Harp and Geoffrey~Irving and Michael~Isard and Yangqing Jia and Rafal~Jozefowicz and Lukasz~Kaiser and Manjunath~Kudlur and Josh~Levenberg and Dandelion~Mane and Rajat~Monga and Sherry~Moore and Derek~Murray and Chris~Olah and Mike~Schuster and Jonathon~Shlens and Benoit~Steiner and Ilya~Sutskever and Kunal~Talwar and Paul~Tucker and Vincent~Vanhoucke and Vijay~Vasudevan and Fernanda~Viegas and Oriol~Vinyals and Pete~Warden and Martin~Wattenberg and Martin~Wicke and Yuan~Yu and Xiaoqiang~Zheng}, year={2015},}