%!TEX root = main.tex

@article{Ronneberger_Fischer_Brox_2015,
    title={U-Net: Convolutional Networks for Biomedical Image Segmentation},
    url={http://arxiv.org/abs/1505.04597},
    author={Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
    year={2015},
    month={05}}
@misc{PET_scan,
    url={https://www.nhs.uk/conditions/pet-scan/},
    title={Positron emission tomography (PET)},
    author={NHS UK}}
@misc{Radiology_ACR,
    title={PET/CT - Positron Emission Tomography/Computed Tomography},
    url={https://www.radiologyinfo.org/en/info.cfm?pg=pet},
    author={Radiological Society of North America (RSNA) and American College of Radiology (ACR)}}
@article{Garipov_Izmailov_Podoprikhin_Vetrov_Wilson_2018, title={Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs}, url={http://arxiv.org/abs/1802.10026}, abstractNote={The loss functions of deep neural networks are complex and their geometric properties are not well understood. We show that the optima of these complex loss functions are in fact connected by simple curves, such as a polygonal chain with only one bend, over which training and test accuracy are nearly constant. We introduce a training procedure to discover these high-accuracy pathways between modes. Inspired by this new geometric insight, we also propose a new ensembling method entitled Fast Geometric Ensembling (FGE). Using FGE we can train high-performing ensembles in the time required to train a single model. We achieve improved performance compared to the recent state-of-the-art Snapshot Ensembles, on CIFAR-10 and CIFAR-100, using state-of-the-art deep residual networks. On ImageNet we improve the top-1 error-rate of a pre-trained ResNet by 0.56\% by running FGE for just 5 epochs.}, note={arXiv: 1802.10026}, journal={arXiv:1802.10026 [cs, stat]}, author={Garipov, Timur and Izmailov, Pavel and Podoprikhin, Dmitrii and Vetrov, Dmitry and Wilson, Andrew Gordon}, year={2018}, month={02}}
@article{Izmailov_Podoprikhin_Garipov_Vetrov_Wilson_2018, title={Averaging Weights Leads to Wider Optima and Better Generalization}, url={http://arxiv.org/abs/1803.05407}, abstractNote={Deep neural networks are typically trained by optimizing a loss function with an SGD variant, in conjunction with a decaying learning rate, until convergence. We show that simple averaging of multiple points along the trajectory of SGD, with a cyclical or constant learning rate, leads to better generalization than conventional training. We also show that this Stochastic Weight Averaging (SWA) procedure finds much broader optima than SGD, and approximates the recent Fast Geometric Ensembling (FGE) approach with a single model. Using SWA we achieve notable improvement in test accuracy over conventional SGD training on a range of state-of-the-art residual networks, PyramidNets, DenseNets, and Shake-Shake networks on CIFAR-10, CIFAR-100, and ImageNet. In short, SWA is extremely easy to implement, improves generalization, and has almost no computational overhead.}, note={arXiv: 1803.05407}, journal={arXiv:1803.05407 [cs, stat]}, author={Izmailov, Pavel and Podoprikhin, Dmitrii and Garipov, Timur and Vetrov, Dmitry and Wilson, Andrew Gordon}, year={2018}, month={03}}
@article{Keskar_Mudigere_Nocedal_Smelyanskiy_Tang_2016, title={On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima}, url={http://arxiv.org/abs/1609.04836}, abstractNote={The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data, say $32$-$512$ data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions - and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We discuss several strategies to attempt to help large-batch methods eliminate this generalization gap.}, note={arXiv: 1609.04836}, journal={arXiv:1609.04836 [cs, math]}, author={Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter}, year={2016}, month={09}}
